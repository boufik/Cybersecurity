This repo contains 4 Python files. Each one of them contains ONLY ONE spider.
**A spider is a mechanism that can scrape and crawl websites in parallel.**

* We need to manually define which fields the crawler will extract through web scraping. We can do so, by manually opening Chrome DevTools of the targeted website and examining the HTML page and the DOM elements inside it.
* In a similar way, we also define how crawling proceeds - we can instruct the crawler to look for `<a>` tags for example.
